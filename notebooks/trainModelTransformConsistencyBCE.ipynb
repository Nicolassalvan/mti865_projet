{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6e025f",
   "metadata": {},
   "source": [
    "# Project MTI865 - Heart segmentation using UNet \n",
    "\n",
    "---\n",
    "\n",
    "# Model training - Transformation consistency with L2 penality \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{n_{l}} \\mathcal{L}_{CE} + \\frac{\\alpha_{TC}}{n_{u}} \\mathcal{L}_{TC-BCE}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b841f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be44f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding .. to path \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6136411",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19e19f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "batch_size_val = 4\n",
    "batch_size_unlabel = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37a65765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image and mask transformations\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "029239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Fonction de regroupement pour le DataLoader.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        batch (list): Liste de tuples (image, masque, chemin de l'image).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        imgs_tensor (torch.Tensor): Batch d'images.\n",
    "        masks_tensor (torch.Tensor): Batch de masques.\n",
    "        img_paths (list): Liste des chemins des images.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    img_paths = []\n",
    "\n",
    "    for item in batch:\n",
    "        img, mask, img_path = item[0], item[1], item[2]\n",
    "        imgs.append(img)\n",
    "        img_paths.append(img_path)\n",
    "        \n",
    "        # Si le masque est None, ajouter un tenseur de zéros correspondant à sa taille\n",
    "        if mask is not None:\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(torch.zeros_like(img[0, :, :]))  # Même taille que le canal de l'image (assumant CxHxW)\n",
    "\n",
    "    # Stack les images et les masques\n",
    "    imgs_tensor = torch.stack(imgs)  # Tensor de forme (B, C, H, W)\n",
    "    masks_tensor = torch.stack(masks)  # Tensor de forme (B, H, W)\n",
    "\n",
    "    return imgs_tensor, masks_tensor, img_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3566dc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset: ../data/ \n",
      "Found 204 items in train\n",
      "First item:  ('../data/train\\\\Img\\\\patient006_01_1.png', '../data/train\\\\GT\\\\patient006_01_1.png')\n",
      "Found 74 items in val\n",
      "First item:  ('../data/val\\\\Img\\\\patient001_01_1.png', '../data/val\\\\GT\\\\patient001_01_1.png')\n",
      "Found 1004 items in train-unlabelled\n",
      "First item:  ('../data/train\\\\Img-Unlabeled\\\\patient007_01_1.png', None)\n",
      "Train set:  204\n",
      "Validation set:  74\n",
      "Image shape:  torch.Size([1, 256, 256])\n",
      "Mask shape:  torch.Size([1, 256, 256])\n",
      "Number of batches:  102\n",
      "Image shape:  torch.Size([1, 256, 256])\n",
      "Mask shape:  torch.Size([1, 256, 256])\n",
      "Number of batches:  19\n",
      "Image shape:  torch.Size([1, 256, 256])\n",
      "Mask shape:  torch.Size([1, 256, 256])\n",
      "Number of batches:  126\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "root_dir = '../data/'\n",
    "print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "supervised_set = medicalDataLoader.MedicalImageDataset(\n",
    "    'train',\n",
    "    root_dir,\n",
    "    transform=transform,\n",
    "    mask_transform=mask_transform,\n",
    "    augment=True,\n",
    "    equalize=False)\n",
    "\n",
    "\n",
    "supervised_loader = DataLoader(\n",
    "    supervised_set,\n",
    "    batch_size=batch_size,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "val_set = medicalDataLoader.MedicalImageDataset(\n",
    "    'val',  \n",
    "    root_dir,\n",
    "    transform=transform,\n",
    "    mask_transform=mask_transform,\n",
    "    equalize=False)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size_val,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=False)\n",
    "\n",
    "unsupervised_set = medicalDataLoader.MedicalImageDataset(\n",
    "    'train-unlabelled',\n",
    "    root_dir,\n",
    "    transform=transform,\n",
    "    mask_transform=mask_transform,\n",
    "    augment=False,\n",
    "    equalize=False)\n",
    "\n",
    "unsupervised_loader = DataLoader(\n",
    "    unsupervised_set,\n",
    "    batch_size=batch_size_unlabel,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "print('Train set: ', len(supervised_set))\n",
    "print('Validation set: ', len(val_set))\n",
    "\n",
    "n_train_label = len(supervised_set)\n",
    "n_train_unlabel = len(unsupervised_set)\n",
    "\n",
    "# shape of the image a  nd mask\n",
    "img, mask, _ = supervised_set[0]\n",
    "print('Image shape: ', img.shape)\n",
    "print('Mask shape: ', mask.shape)\n",
    "print('Number of batches: ', len(supervised_loader))\n",
    "\n",
    "img, mask, _ = val_set[0]\n",
    "print('Image shape: ', img.shape)\n",
    "print('Mask shape: ', mask.shape)\n",
    "print('Number of batches: ', len(val_loader))\n",
    "\n",
    "img, _, __ = unsupervised_set[0]\n",
    "print('Image shape: ', img.shape)\n",
    "print('Mask shape: ', mask.shape)\n",
    "print('Number of batches: ', len(unsupervised_loader))\n",
    "\n",
    "\n",
    "# print('First of the supervised set')\n",
    "# img, mask, path_tuple = supervised_set[0]\n",
    "# print(img)\n",
    "# print(mask)\n",
    "# print(path_tuple)\n",
    "\n",
    "# print('First of the unsupervised set')\n",
    "# img, mask, path_tuple = unsupervised_set[0]\n",
    "# print(img)\n",
    "# print(mask)\n",
    "# print(path_tuple)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7c545",
   "metadata": {},
   "source": [
    "## Model using both labeled and unlabeled data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31076c",
   "metadata": {},
   "source": [
    "### Hyperparameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "369d8a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters saved to models/TransformationConsistencyBCEModel-150epochs0.001lr0.1alphaTC1e-05wd/params.txt\n"
     ]
    }
   ],
   "source": [
    "# Parameters \n",
    "lr =  0.001    # Learning Rate\n",
    "total_epochs = 150  # Number of epochs\n",
    "alpha_TC = 0.1 # Alpha parameter for the consistency loss term \n",
    "weight_decay = 1e-5  # Weight decay\n",
    "\n",
    "param_dict = {\n",
    "    'model': 'UNet Transformed Consistency',\n",
    "    'lr': lr,\n",
    "    'total_epochs': total_epochs,\n",
    "    'alpha_TC': alpha_TC,\n",
    "    'weight_decay': weight_decay,\n",
    "    'batch_size': batch_size,\n",
    "    'batch_size_val': batch_size_val,\n",
    "    'batch_size_unlabel': batch_size_unlabel,\n",
    "    'consistency_criterion': 'BCE'\n",
    "}\n",
    "modelName = f\"TransformationConsistencyBCEModel-{total_epochs}epochs{lr}lr{alpha_TC}alphaTC{weight_decay}wd\"\n",
    "model_dir = f\"models/{modelName}\"\n",
    "# write params in a file \n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "with open(f\"{model_dir}/params.txt\", 'w') as f:\n",
    "    print(param_dict, file=f)\n",
    "\n",
    "print(f\"Parameters saved to {model_dir}/params.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c94cc8",
   "metadata": {},
   "source": [
    "### Example of loop\n",
    "\n",
    "In this exemple, we modified the structure of the code to use iterators instead. In each epoch, we see the whole range of supervised data and one time each unsupervised data. We can also try to see once the supervised data and to see random images of unsupervised data, which could mean we would not be able to see it all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e05de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised_iter = iter(supervised_loader)\n",
    "# unsupervised_iter = iter(unsupervised_loader)\n",
    "\n",
    "# for epoch in range(total_epochs):\n",
    "#     num_batches = max(len(supervised_loader), len(unsupervised_loader))\n",
    "#     for idx in range(num_batches):\n",
    "#         try :\n",
    "#             supervised_data = next(supervised_iter)\n",
    "#         except StopIteration:\n",
    "#             supervised_iter = iter(supervised_loader)\n",
    "#             supervised_data= next(supervised_iter)\n",
    "\n",
    "#         print(supervised_data)\n",
    "#         print('Supervised batch')   \n",
    "#         try :\n",
    "#             unsupervised_data = next(unsupervised_iter)\n",
    "#         except StopIteration:\n",
    "#             unsupervised_iter = iter(unsupervised_loader)\n",
    "#             unsupervised_data = next(unsupervised_iter)\n",
    "        \n",
    "#         print(unsupervised_data)\n",
    "#         print('Unsupervised batch')\n",
    "#         break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf0784",
   "metadata": {},
   "source": [
    "### Transformation consistency regularisation\n",
    "\n",
    "The transformation consistency consists in the principle that transformation T suchs as rotation and flipping should affect the mask f(y) only by the same rotation, which means that f and T should be symetrical. In this implementation, we used the 2-norm to measure the difference, and we included it in the optimisation problem.  $\\mathcal{L}_{TC}(y_u) = \\|f(T(y_u))-T(F(y))\\|_2$. \n",
    "Il est aussi possible de faire une régularisation avec la CE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16443fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class ConsistencyRegularization(nn.Module):\n",
    "    def __init__(self, transformation_fn, loss_fn=nn.MSELoss()):\n",
    "        \"\"\"\n",
    "        Régularisation basée sur la consistance à la transformation.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "            transformation_fn (callable): Fonction d'augmentation/transformation appliquée aux images.\n",
    "            loss_fn (callable): Fonction de perte utilisée pour comparer les prédictions (par défaut MSELoss). Aussi possible d'utiliser \n",
    "                                nn.KLDivLoss ou nn.BCELoss.\n",
    "        \"\"\"\n",
    "        super(ConsistencyRegularization, self).__init__()\n",
    "        self.transformation_fn = transformation_fn\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def forward(self, model, images):\n",
    "        \"\"\"\n",
    "        Calcule la perte de consistance.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Le modèle de segmentation.\n",
    "            images (torch.Tensor): Batch d'images d'entrée.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: La perte de consistance.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Prédictions de base\n",
    "            original_predictions = F.softmax(model(images), dim=1)\n",
    "\n",
    "        # Augmenter les images\n",
    "        augmented_images = self.transformation_fn(images)\n",
    "\n",
    "        # Prédictions pour les images augmentées\n",
    "        augmented_predictions = F.softmax(model(augmented_images), dim=1)\n",
    "        if self.loss_fn == nn.BCELoss():\n",
    "            original_predictions = original_predictions[:, 1, :, :].unsqueeze(1) # Garder uniquement le canal de segmentation \n",
    "            augmented_predictions = augmented_predictions[:, 1, :, :].unsqueeze(1) # Garder uniquement le canal de segmentation\n",
    "\n",
    "        # Calcul de la perte de consistance\n",
    "        consistency_loss = self.loss_fn(original_predictions, augmented_predictions)\n",
    "\n",
    "        return consistency_loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "657ddeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transformation_fn(images):\n",
    "  \n",
    "    # Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        images = torch.flip(images, dims=[2])\n",
    "    # Random vertical flip\n",
    "    if np.random.random() > 0.5:\n",
    "        images = torch.flip(images, dims=[3])\n",
    "    # Random rotation of random angle\n",
    "    if np.random.random() > 0.5:\n",
    "        angle = np.random.randint(0, 360)\n",
    "        images = torch.rot90(images, k=angle//90, dims=[2, 3])\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f5c928",
   "metadata": {},
   "source": [
    "### Training of the model \n",
    "\n",
    "At each epoch, the model sees once every exemple of unlabeled data, and sees several time the labeled data. We first train it with the labeled data, and then we train it on the unsupervised data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "186cae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      "Using device: cpu\n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name: TransformationConsistencyBCEModel-150epochs0.001lr0.1alphaTC1e-05wd\n",
      "Total params: 60,664\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "Number of batches:  126\n",
      "[Training] Epoch: 0 [DONE]                                 \n",
      "[Validation] Epoch: 0 [DONE]                                 \n",
      "[Epoch: 0, TrainLoss: 2.4271, TrainDice: 0.0538, ValLoss: 2.1828                                             \n",
      "Number of batches:  126\n",
      "[Training] Epoch: 1 [===>           ] 22.2% Loss: 2.7021, "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32924\\3022070564.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# optimizer.zero_grad()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mconsistency_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconsistency_regularizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsupervised_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0malpha_TC\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mconsistency_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32924\\3149528261.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, model, images)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# Calcul de la perte de consistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mconsistency_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maugmented_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconsistency_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nicos\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3170\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3172\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"~~~~~~~~  Starting the training... ~~~~~~\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "# Set device depending on the availability of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.mps.is_available():  # Apple M-series of chips\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "print(\" Model Name: {}\".format(modelName))\n",
    "\n",
    "## CREATION OF YOUR MODEL\n",
    "net = UNet(num_classes).to(device)\n",
    "\n",
    "print(\n",
    "    \"Total params: {0:,}\".format(\n",
    "        sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    )\n",
    ")\n",
    "\n",
    "# DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "softMax = torch.nn.Softmax(dim=1)\n",
    "CE_loss = torch.nn.CrossEntropyLoss()\n",
    "consistency_regularizer = ConsistencyRegularization(transformation_fn=random_transformation_fn, loss_fn=torch.nn.BCELoss())\n",
    "\n",
    "\n",
    "## PUT EVERYTHING IN GPU RESOURCES\n",
    "# if torch.cuda.is_available():\n",
    "#     net.cuda()\n",
    "#     softMax.cuda()\n",
    "#     CE_loss.cuda()\n",
    "\n",
    "## DEFINE YOUR OPTIMIZER\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "### To save statistics ####\n",
    "train_losses = []\n",
    "train_dc_losses = []\n",
    "val_losses = []\n",
    "val_dc_losses = []\n",
    "\n",
    "best_loss_val = 1000\n",
    "\n",
    "directory = \"Results/Statistics/\" + modelName\n",
    "\n",
    "print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "if os.path.exists(directory) == False:\n",
    "    os.makedirs(directory)\n",
    "\n",
    "## START THE TRAINING\n",
    "\n",
    "## FOR EACH EPOCH\n",
    "for epoch in range(total_epochs):\n",
    "    net.train()\n",
    "    supervised_iter = iter(supervised_loader)\n",
    "    unsupervised_iter = iter(unsupervised_loader)\n",
    "    \n",
    "    num_batches = max(len(supervised_loader), len(unsupervised_loader))\n",
    "    print(\"Number of batches: \", num_batches)\n",
    "\n",
    "    running_train_loss = 0\n",
    "    running_dice_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for idx in range(num_batches):\n",
    "        ### SUPERVISED BATCH\n",
    "        try :\n",
    "            supervised_data = next(supervised_iter)\n",
    "        except StopIteration:\n",
    "            supervised_iter = iter(supervised_loader)\n",
    "            supervised_data = next(supervised_iter)\n",
    "\n",
    "        ### Set to zero all the gradients\n",
    "        net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ## GET IMAGES, LABELS and IMG NAMES\n",
    "        images, labels, img_names = supervised_data\n",
    "\n",
    "        ### From numpy to torch variables\n",
    "        labels = utils.to_var(labels).to(device)\n",
    "        images = utils.to_var(images).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        net_predictions = net(images)\n",
    "\n",
    "        # Get the segmentation classes\n",
    "        segmentation_classes = utils.getTargetSegmentation(labels)\n",
    "\n",
    "        # Compute the loss\n",
    "        ce_loss = CE_loss(net_predictions, segmentation_classes)  \n",
    "        running_train_loss += ce_loss.item()\n",
    "        # dice_loss = dice_coefficient(net_predictions, labels)\n",
    "        dice_loss = utils.compute_dsc(net_predictions, labels)\n",
    "        running_dice_loss += dice_loss\n",
    "\n",
    "        # Backprop\n",
    "        # ce_loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        ### UNSUPERVISED BATCH\n",
    "        try :\n",
    "            unsupervised_data = next(unsupervised_iter)\n",
    "        except StopIteration:\n",
    "            unsupervised_iter = iter(unsupervised_loader)\n",
    "            unsupervised_data = next(unsupervised_iter)\n",
    "        \n",
    "        unsupervised_images, _, _ = unsupervised_data\n",
    "        unsupervised_images = utils.to_var(unsupervised_images).to(device)\n",
    "\n",
    "        # net.zero_grad()\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        consistency_loss = consistency_regularizer(net, unsupervised_images) \n",
    "        (alpha_TC * consistency_loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += consistency_loss.item()\n",
    "        running_dice_loss += 0\n",
    "\n",
    "        # Add the loss to the tensorboard every 5 batches\n",
    "        if idx % 10 == 0:\n",
    "            writer.add_scalar(\n",
    "                \"Loss/train\", running_train_loss / (idx + 1), epoch * len(supervised_loader) + idx\n",
    "            )\n",
    "            writer.add_scalar(\n",
    "                \"Dice/train\", running_dice_loss / (idx + 1), epoch * len(supervised_loader) + idx\n",
    "            )\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            # Also add visualizations of the images\n",
    "            probs = torch.softmax(net_predictions, dim=1)\n",
    "            y_pred = torch.argmax(probs, dim=1)\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                        utils.plot_net_predictions(images, labels, y_pred, batch_size),\n",
    "                        global_step=epoch * len(supervised_loader) + idx)\n",
    "\n",
    "        # THIS IS JUST TO VISUALIZE THE TRAINING\n",
    "        printProgressBar(\n",
    "            idx + 1,\n",
    "            num_batches,\n",
    "            prefix=\"[Training] Epoch: {} \".format(epoch),\n",
    "            length=15,\n",
    "            suffix=\" Loss: {:.4f}, \".format(running_train_loss / (idx + 1)),\n",
    "        )\n",
    "\n",
    "    train_loss = running_train_loss / num_batches\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    train_dc_loss = running_dice_loss / num_batches\n",
    "    train_dc_losses.append(train_dc_loss)\n",
    "\n",
    "    net.eval()\n",
    "    val_running_loss = 0\n",
    "    val_running_dc = 0\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(val_loader):\n",
    "            images, labels, img_names = data\n",
    "\n",
    "            labels = utils.to_var(labels).to(device)\n",
    "            images = utils.to_var(images).to(device)\n",
    "\n",
    "            net_predictions = net(images)\n",
    "\n",
    "            segmentation_classes = utils.getTargetSegmentation(labels)\n",
    "\n",
    "            loss = CE_loss(net_predictions, segmentation_classes) \n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # dice_loss = dice_coefficient(net_predictions, labels)\n",
    "            dice_loss = utils.compute_dsc(net_predictions, labels)\n",
    "            val_running_dc += dice_loss\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                writer.add_scalar(\n",
    "                    \"Loss/val\",\n",
    "                    val_running_loss / (idx + 1),\n",
    "                    epoch * len(val_loader) + idx,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"Dice/val\",\n",
    "                    val_running_dc / (idx + 1),\n",
    "                    epoch * len(val_loader) + idx,\n",
    "                )\n",
    "\n",
    "            printProgressBar(\n",
    "                idx + 1,\n",
    "                len(val_loader),\n",
    "                prefix=\"[Validation] Epoch: {} \".format(epoch),\n",
    "                length=15,\n",
    "                suffix=\" Loss: {:.4f}, \".format(val_running_loss / (idx + 1)),\n",
    "            )\n",
    "\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    dc_loss = val_running_dc / len(val_loader)\n",
    "    val_dc_losses.append(dc_loss)\n",
    "\n",
    "    # Check if model performed best and save it if true\n",
    "    if val_loss < best_loss_val:\n",
    "        best_loss_val = val_loss\n",
    "        if not os.path.exists(\"./models/\" + modelName):\n",
    "            os.makedirs(\"./models/\" + modelName)\n",
    "        torch.save(\n",
    "            net.state_dict(), \"./models/\" + modelName + \"/\" + str(epoch) + \"_Epoch\"\n",
    "        )\n",
    "\n",
    "    printProgressBar(\n",
    "        num_batches,\n",
    "        num_batches,\n",
    "        done=\"[Epoch: {}, TrainLoss: {:.4f}, TrainDice: {:.4f}, ValLoss: {:.4f}\".format(\n",
    "            epoch, train_loss, train_dc_loss, val_loss\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    np.save(os.path.join(directory, \"Losses.npy\"), train_losses)\n",
    "writer.flush()  # Flush the writer to ensure that all the data is written to disk\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
