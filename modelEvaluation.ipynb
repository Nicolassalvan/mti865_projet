{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project MTI865 - Heart segmentation using UNet \n",
    "\n",
    "---\n",
    "\n",
    "# Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2\n",
    "from progressBar import printProgressBar\n",
    "import torchvision\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "from scipy import stats \n",
    "import metrics \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch_size_val = 4\n",
    "batch_size_unlabel = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask and image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image and mask transformations\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    img_paths = []\n",
    "\n",
    "    for item in batch:\n",
    "        img, mask, img_path = item[0], item[1], item[2]\n",
    "        imgs.append(img)\n",
    "        img_paths.append(img_path)\n",
    "        \n",
    "        # Si le masque est None, ajouter un tenseur de zéros correspondant à sa taille\n",
    "        if mask is not None:\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(torch.zeros_like(img[0, :, :]))  # Même taille que le canal de l'image (assumant CxHxW)\n",
    "\n",
    "    # Stack les images et les masques\n",
    "    imgs_tensor = torch.stack(imgs)  # Tensor de forme (B, C, H, W)\n",
    "    masks_tensor = torch.stack(masks)  # Tensor de forme (B, H, W)\n",
    "\n",
    "    return imgs_tensor, masks_tensor, img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset: ./data/ \n",
      "Found 204 items in train\n",
      "First item:  ('./data/train\\\\Img\\\\patient006_01_1.png', './data/train\\\\GT\\\\patient006_01_1.png')\n",
      "Found 74 items in val\n",
      "First item:  ('./data/val\\\\Img\\\\patient001_01_1.png', './data/val\\\\GT\\\\patient001_01_1.png')\n",
      "Found 1004 items in train-unlabelled\n",
      "First item:  ('./data/train\\\\Img-Unlabeled\\\\patient007_01_1.png', None)\n",
      "Found 314 items in test\n",
      "First item:  ('./data/test\\\\Img\\\\patient002_01_1.png', './data/test\\\\GT\\\\patient002_01_1.png')\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "root_dir = './data/'\n",
    "print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "supervised_set = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "\n",
    "supervised_loader = DataLoader(\n",
    "    supervised_set,\n",
    "    batch_size=batch_size,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "unsupervised_set = medicalDataLoader.MedicalImageDataset('train-unlabelled',\n",
    "                                                            root_dir,\n",
    "                                                            transform=transform,\n",
    "                                                            mask_transform=mask_transform,\n",
    "                                                            augment=False,\n",
    "                                                            equalize=False)\n",
    "# print(train_unlabelled_set.imgs)\n",
    "# train_unlabelled_set = [(img) for img, mask in train_unlabelled_set]\n",
    "unsupervised_loader = DataLoader(unsupervised_set,\n",
    "                                    batch_size=batch_size_unlabel,\n",
    "                                    worker_init_fn=np.random.seed(0),\n",
    "                                    num_workers=0,\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=collate_fn)\n",
    "\n",
    "test_set = medicalDataLoader.MedicalImageDataset('test',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size_unlabel,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Let's print the first batch to understand the data\n",
    "\n",
    "for loader in [supervised_loader, val_loader, unsupervised_loader, test_loader]:\n",
    "    imgs, masks, img_paths = next(iter(loader))\n",
    "    print('Images shape: ', imgs.shape)\n",
    "    print('Masks shape: ', masks.shape)\n",
    "    # print('Image paths: ', img_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.mps.is_available():  # Apple M-series of chips\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epoch_to_load = 93\n",
    "model = UNet(4).to(device=device)\n",
    "modelName = 'Test_Model'\n",
    "# model.load_state_dict(torch.load(f\"./models/{modelName}/{epoch_to_load}_Epoch\"))\n",
    "\n",
    "model.load_state_dict(torch.load(\"save/TC-L2/141_Epoch-0.1TC_L2\"))\n",
    "# model.load_state_dict(torch.load(f\"./models/SemiSupervised-TransformConsistency/{epoch_to_load}_Epoch\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inferenceEval() missing 1 required positional argument: 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37004\\2204659396.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0minf_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minferenceEval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"best_model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: inferenceEval() missing 1 required positional argument: 'epoch'"
     ]
    }
   ],
   "source": [
    "def inferenceEval(net, img_batch, modelName):\n",
    "    \"\"\"\n",
    "    Function to perform inference on a batch of images and save the results\n",
    "    \n",
    "    \"\"\"\n",
    "    epoch=0\n",
    "    total = len(img_batch)\n",
    "    net.eval()\n",
    "\n",
    "    softMax = nn.Softmax().cuda()\n",
    "    CE_loss = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    losses = []\n",
    "    for i, data in enumerate(img_batch):\n",
    "\n",
    "        printProgressBar(\n",
    "            i, total, prefix=\"[Inference] Getting segmentations...\", length=30\n",
    "        )\n",
    "        images, labels, img_names = data\n",
    "\n",
    "        images = utils.to_var(images)\n",
    "        labels = utils.to_var(labels)\n",
    "\n",
    "        net_predictions = net(images)\n",
    "        print(net_predictions.shape)\n",
    "        segmentation_classes = utils.getTargetSegmentation(labels)\n",
    "        CE_loss_value = CE_loss(net_predictions, segmentation_classes)\n",
    "        losses.append(CE_loss_value.cpu().data.numpy())\n",
    "        pred_y = softMax(net_predictions)\n",
    "        masks = torch.argmax(pred_y, dim=1)\n",
    "\n",
    "        path = os.path.join(\"./Results/Images/\", modelName, str(epoch))\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        torchvision.utils.save_image(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    images.data,\n",
    "                    labels.data,\n",
    "                    masks.view(labels.shape[0], 1, 256, 256).data / 3.0,\n",
    "                ]\n",
    "            ),\n",
    "            os.path.join(path, str(i) + \".png\"),\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "    printProgressBar(total, total, done=\"[Inference] Segmentation Done !\")\n",
    "\n",
    "    losses = np.asarray(losses)\n",
    "\n",
    "    return losses.mean()\n",
    "\n",
    "inf_losses = inferenceEval(model, val_loader, \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dice_score_array = metrics.Dice_score_class(model, test_loader, device)\n",
    "iou_score_array = metrics.Jaccard_score_class(model, test_loader, device)\n",
    "hsd_score_array = metrics.hsd_score_class(model, test_loader, device)\n",
    "precision_score_array = metrics.precision_class(model, test_loader, device)\n",
    "recall_score_array = metrics.recall_class(model, test_loader, device)\n",
    "f1_score_array = metrics.f1_score_class(model, test_loader, device)\n",
    "auc_score_array = metrics.auc_coeff_class(model, test_loader, device)\n",
    "accuracy_score_array = metrics.accuracy_class(model, test_loader, device)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice score:  [0.9951914279274305, 0.45760283841515875, 0.47520756224314215, 0.6613782354573153]\n",
      "[0.45760283841515875, 0.47520756224314215, 0.6613782354573153]\n"
     ]
    }
   ],
   "source": [
    "print('Dice score: ', dice_score_array)\n",
    "print(dice_score_array[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_mean = np.mean(dice_score_array)\n",
    "iou_mean = np.mean(iou_score_array)\n",
    "hsd_mean = np.mean(hsd_score_array)\n",
    "precision_mean = np.mean(precision_score_array)\n",
    "recall_mean = np.mean(recall_score_array)\n",
    "f1_mean = np.mean(f1_score_array)\n",
    "auc_mean = np.mean(auc_score_array)\n",
    "accuracy_mean = np.mean(accuracy_score_array)\n",
    "\n",
    "dice_mean_without_bg = np.mean(dice_score_array[1:])\n",
    "iou_mean_without_bg = np.mean(iou_score_array[1:])\n",
    "hsd_mean_without_bg = np.mean(hsd_score_array[1:])\n",
    "precision_mean_without_bg = np.mean(precision_score_array[1:])\n",
    "recall_mean_without_bg = np.mean(recall_score_array[1:])\n",
    "f1_mean_without_bg = np.mean(f1_score_array[1:])\n",
    "auc_mean_without_bg = np.mean(auc_score_array[1:])\n",
    "accuracy_mean_without_bg = np.mean(accuracy_score_array[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# building the data frame \n",
    "data = {'Dice': dice_mean, 'IoU': iou_mean, 'HSD': hsd_mean, 'Precision': precision_mean, 'Recall': recall_mean, 'F1': f1_mean, 'AUC': auc_mean, 'Accuracy': accuracy_mean}\n",
    "data_without_bg = {'Dice': dice_mean_without_bg, 'IoU': iou_mean_without_bg, 'HSD': hsd_mean_without_bg, 'Precision': precision_mean_without_bg, 'Recall': recall_mean_without_bg, 'F1': f1_mean_without_bg, 'AUC': auc_mean_without_bg, 'Accuracy': accuracy_mean_without_bg}\n",
    "\n",
    "# Create DataFrame\n",
    "df_mean = pd.DataFrame(data, index =['Mean'])\n",
    "df_without_bg = pd.DataFrame(data_without_bg, index =['Mean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dice       IoU        HSD  Precision    Recall        F1       AUC  \\\n",
      "Mean  0.647345  0.571439  14.057255    0.70204  0.650425  0.647816  0.742978   \n",
      "\n",
      "      Accuracy  \n",
      "Mean  0.993596  \n"
     ]
    }
   ],
   "source": [
    "print(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dice       IoU        HSD  Precision    Recall        F1       AUC  \\\n",
      "Mean  0.531396  0.431582  10.332958   0.604708  0.535026  0.532009  0.716499   \n",
      "\n",
      "      Accuracy  \n",
      "Mean  0.994648  \n"
     ]
    }
   ],
   "source": [
    "print(df_without_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Dice       IoU        HSD  Precision    Recall        F1  \\\n",
      "Background  0.995191  0.991008  25.230146   0.994036  0.996624  0.995238   \n",
      "RV          0.457603  0.369323  30.998874   0.474567  0.484988  0.459469   \n",
      "Myo         0.475208  0.347397   0.000000   0.605138  0.433072  0.471277   \n",
      "LV          0.661378  0.578027   0.000000   0.734420  0.687016  0.665280   \n",
      "\n",
      "                 AUC  Accuracy  \n",
      "Background  0.822413  0.990440  \n",
      "RV          0.659099  0.993669  \n",
      "Myo         0.687253  0.993352  \n",
      "LV          0.803146  0.996922  \n"
     ]
    }
   ],
   "source": [
    "data_by_class = {'Dice': dice_score_array, 'IoU': iou_score_array, 'HSD': hsd_score_array, 'Precision': precision_score_array, 'Recall': recall_score_array, 'F1': f1_score_array, 'AUC': auc_score_array, 'Accuracy': accuracy_score_array}\n",
    "index = ['Background', 'RV', 'Myo', 'LV']\n",
    "df_by_class = pd.DataFrame(data_by_class, index=index)\n",
    "print(df_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv \n",
    "df_mean.to_csv('mean_metrics.csv')\n",
    "df_without_bg.to_csv('mean_metrics_without_bg.csv')\n",
    "df_by_class.to_csv('metrics_by_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
