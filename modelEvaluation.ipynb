{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project MTI865 - Heart segmentation using UNet \n",
    "\n",
    "---\n",
    "\n",
    "# Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "from scipy import stats \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "batch_size_val = 1\n",
    "batch_size_unlabel = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask and image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image and mask transformations\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    img_paths = []\n",
    "\n",
    "    for item in batch:\n",
    "        img, mask, img_path = item[0], item[1], item[2]\n",
    "        imgs.append(img)\n",
    "        img_paths.append(img_path)\n",
    "        \n",
    "        # Si le masque est None, ajouter un tenseur de zéros correspondant à sa taille\n",
    "        if mask is not None:\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(torch.zeros_like(img[0, :, :]))  # Même taille que le canal de l'image (assumant CxHxW)\n",
    "\n",
    "    # Stack les images et les masques\n",
    "    imgs_tensor = torch.stack(imgs)  # Tensor de forme (B, C, H, W)\n",
    "    masks_tensor = torch.stack(masks)  # Tensor de forme (B, H, W)\n",
    "\n",
    "    return imgs_tensor, masks_tensor, img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders\n",
    "root_dir = './data/'\n",
    "print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "supervised_set = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "\n",
    "supervised_loader = DataLoader(\n",
    "    supervised_set,\n",
    "    batch_size=batch_size,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "unsupervised_set = medicalDataLoader.MedicalImageDataset('train-unlabelled',\n",
    "                                                            root_dir,\n",
    "                                                            transform=transform,\n",
    "                                                            mask_transform=mask_transform,\n",
    "                                                            augment=False,\n",
    "                                                            equalize=False)\n",
    "# print(train_unlabelled_set.imgs)\n",
    "# train_unlabelled_set = [(img) for img, mask in train_unlabelled_set]\n",
    "unsupervised_loader = DataLoader(unsupervised_set,\n",
    "                                    batch_size=batch_size_unlabel,\n",
    "                                    worker_init_fn=np.random.seed(0),\n",
    "                                    num_workers=0,\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=collate_fn)\n",
    "\n",
    "test_set = medicalDataLoader.MedicalImageDataset('test',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size_unlabel,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Let's print the first batch to understand the data\n",
    "\n",
    "for loader in [supervised_loader, val_loader, unsupervised_loader, test_loader]:\n",
    "    imgs, masks, img_paths = next(iter(loader))\n",
    "    print('Images shape: ', imgs.shape)\n",
    "    print('Masks shape: ', masks.shape)\n",
    "    # print('Image paths: ', img_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.mps.is_available():  # Apple M-series of chips\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epoch_to_load = 96\n",
    "model = UNet(4).to(device=device)\n",
    "modelName = 'Test_Model'\n",
    "model.load_state_dict(torch.load(f\"./models/{modelName}/{epoch_to_load}_Epoch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(val_loader)\n",
    "num_batch = len(val_loader)\n",
    "print('Number of batches: ', num_batch)\n",
    "\n",
    "for i in range(num_batch):\n",
    "    img, mask, path_img = next(data_iterator)\n",
    "    print(path_img)\n",
    "    img = utils.to_var(img)\n",
    "    mask = utils.to_var(mask)\n",
    "\n",
    "    print(f\"Image : shape {img.shape}, stats : {stats.describe(img.flatten().cpu().numpy())}\")\n",
    "    img = img.to(device=device).detach()\n",
    "    # print(f\"Image : shape {img.shape}, stats : {stats.describe(img.flatten().cpu().numpy())}\")\n",
    "    mask = mask.to(device=device).detach()\n",
    "    # print(f\"Mask : shape {mask.shape}, stats : {stats.describe(mask.flatten().cpu().numpy())}\")\n",
    "    pred = model(img).detach()\n",
    "    print(f\"Pred : shape {pred.shape}, stats : {stats.describe(pred.flatten().cpu().numpy())}\")\n",
    "\n",
    "    probs = torch.softmax(pred, dim=1).detach()\n",
    "    print(f\"Probs : shape {probs.shape}, stats : {stats.describe(probs.flatten().cpu().numpy())}\")\n",
    "    y_pred = torch.argmax(probs, dim=1).detach()\n",
    "    print(f\"y_pred : shape {y_pred.shape}, stats : {stats.describe(y_pred.flatten().cpu().numpy())}\") \n",
    "\n",
    "    y_true = utils.getTargetSegmentation(mask)\n",
    "    print(f\"y_true : shape {y_true.shape}, stats : {stats.describe(y_true.flatten().cpu().numpy())}\")\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    # Convert predictions and true values to numpy arrays \n",
    "    y_pred = y_pred.cpu().numpy()[0]\n",
    "    y_true = y_true.cpu().numpy()\n",
    "\n",
    "    # print(f\"y_pred : shape {y_pred.shape}\")\n",
    "    # print(f\"y_true : shape {y_true.shape}\")\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5)) \n",
    "    ax[0].imshow(img[0, 0, :, :], cmap='gray')\n",
    "    ax[0].set_title('Image')\n",
    "    ax[1].imshow(y_true, cmap='gray')\n",
    "    ax[1].set_title('Ground Truth')\n",
    "    ax[2].imshow(y_pred, cmap='gray')\n",
    "    ax[2].set_title('Prediction')\n",
    "    plt.show()\n",
    "    if i> 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the confusion matrix for one image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(val_loader)\n",
    "num_batch = len(val_loader)\n",
    "print('Number of batches: ', num_batch)\n",
    "\n",
    "for i in range(num_batch):\n",
    "    img, mask, _ = next(data_iterator)\n",
    "    print(f\"Image : shape {img.shape}, stats : {stats.describe(img.flatten().cpu().numpy())}\")\n",
    "    img = img.to(device=device).detach()\n",
    "    # print(f\"Image : shape {img.shape}, stats : {stats.describe(img.flatten().cpu().numpy())}\")\n",
    "    mask = mask.to(device=device).detach()\n",
    "    # print(f\"Mask : shape {mask.shape}, stats : {stats.describe(mask.flatten().cpu().numpy())}\")\n",
    "    pred = model(img).detach()\n",
    "    print(f\"Pred : shape {pred.shape}, stats : {stats.describe(pred.flatten().cpu().numpy())}\")\n",
    "\n",
    "    probs = torch.softmax(pred, dim=1).detach()\n",
    "    # print(f\"Probs : shape {probs.shape}, stats : {stats.describe(probs.flatten().cpu().numpy())}\")\n",
    "    y_pred = torch.argmax(probs, dim=1).detach()\n",
    "    print(f\"y_pred : shape {y_pred.shape}, stats : {stats.describe(y_pred.flatten().cpu().numpy())}\") \n",
    "\n",
    "    y_true = utils.getTargetSegmentation(mask)\n",
    "    # print(f\"y_true : shape {y_true.shape}, stats : {stats.describe(y_true.flatten().cpu().numpy())}\")\n",
    "\n",
    "    # Convert predictions and true values to numpy arrays \n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    y_true = y_true.cpu().numpy().flatten()\n",
    "\n",
    "    print(f\"y_pred : shape {y_pred.shape}, stats : {stats.describe(y_pred)}\")\n",
    "    print(f\"y_true : shape {y_true.shape}, stats : {stats.describe(y_true)}\")\n",
    "    try : \n",
    "        confusionMatrix += skmetrics.confusion_matrix(y_true, y_pred)\n",
    "    except :\n",
    "        confusionMatrix = skmetrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(confusionMatrix)\n",
    "\n",
    "normalizedConfusionMatrix = confusionMatrix / confusionMatrix.sum(axis=1)[:, np.newaxis]\n",
    "print(normalizedConfusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
