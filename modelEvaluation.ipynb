{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project MTI865 - Heart segmentation using UNet \n",
    "\n",
    "---\n",
    "\n",
    "# Model evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import v2\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn import metrics as skmetrics\n",
    "from scipy import stats \n",
    "import metrics \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch_size_val = 4\n",
    "batch_size_unlabel = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask and image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image and mask transformations\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = v2.Compose([\n",
    "    v2.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "    img_paths = []\n",
    "\n",
    "    for item in batch:\n",
    "        img, mask, img_path = item[0], item[1], item[2]\n",
    "        imgs.append(img)\n",
    "        img_paths.append(img_path)\n",
    "        \n",
    "        # Si le masque est None, ajouter un tenseur de zéros correspondant à sa taille\n",
    "        if mask is not None:\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(torch.zeros_like(img[0, :, :]))  # Même taille que le canal de l'image (assumant CxHxW)\n",
    "\n",
    "    # Stack les images et les masques\n",
    "    imgs_tensor = torch.stack(imgs)  # Tensor de forme (B, C, H, W)\n",
    "    masks_tensor = torch.stack(masks)  # Tensor de forme (B, H, W)\n",
    "\n",
    "    return imgs_tensor, masks_tensor, img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset: ./data/ \n",
      "Found 204 items in train\n",
      "First item:  ('./data/train\\\\Img\\\\patient006_01_1.png', './data/train\\\\GT\\\\patient006_01_1.png')\n",
      "Found 74 items in val\n",
      "First item:  ('./data/val\\\\Img\\\\patient001_01_1.png', './data/val\\\\GT\\\\patient001_01_1.png')\n",
      "Found 1004 items in train-unlabelled\n",
      "First item:  ('./data/train\\\\Img-Unlabeled\\\\patient007_01_1.png', None)\n",
      "Found 314 items in test\n",
      "First item:  ('./data/test\\\\Img\\\\patient002_01_1.png', './data/test\\\\GT\\\\patient002_01_1.png')\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 256, 256])\n",
      "Images shape:  torch.Size([4, 1, 256, 256])\n",
      "Masks shape:  torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders\n",
    "root_dir = './data/'\n",
    "print(' Dataset: {} '.format(root_dir))\n",
    "\n",
    "supervised_set = medicalDataLoader.MedicalImageDataset('train',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "\n",
    "supervised_loader = DataLoader(\n",
    "    supervised_set,\n",
    "    batch_size=batch_size,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "val_set = medicalDataLoader.MedicalImageDataset('val',\n",
    "                                                root_dir,\n",
    "                                                transform=transform,\n",
    "                                                mask_transform=mask_transform,\n",
    "                                                equalize=False)\n",
    "\n",
    "val_loader = DataLoader(val_set,\n",
    "                        batch_size=batch_size_val,\n",
    "                        worker_init_fn=np.random.seed(0),\n",
    "                        num_workers=0,\n",
    "                        shuffle=False)\n",
    "\n",
    "unsupervised_set = medicalDataLoader.MedicalImageDataset('train-unlabelled',\n",
    "                                                            root_dir,\n",
    "                                                            transform=transform,\n",
    "                                                            mask_transform=mask_transform,\n",
    "                                                            augment=False,\n",
    "                                                            equalize=False)\n",
    "# print(train_unlabelled_set.imgs)\n",
    "# train_unlabelled_set = [(img) for img, mask in train_unlabelled_set]\n",
    "unsupervised_loader = DataLoader(unsupervised_set,\n",
    "                                    batch_size=batch_size_unlabel,\n",
    "                                    worker_init_fn=np.random.seed(0),\n",
    "                                    num_workers=0,\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=collate_fn)\n",
    "\n",
    "test_set = medicalDataLoader.MedicalImageDataset('test',\n",
    "                                                    root_dir,\n",
    "                                                    transform=transform,\n",
    "                                                    mask_transform=mask_transform,\n",
    "                                                    augment=True,\n",
    "                                                    equalize=False)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_size=batch_size_unlabel,\n",
    "    worker_init_fn=np.random.seed(0),\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Let's print the first batch to understand the data\n",
    "\n",
    "for loader in [supervised_loader, val_loader, unsupervised_loader, test_loader]:\n",
    "    imgs, masks, img_paths = next(iter(loader))\n",
    "    print('Images shape: ', imgs.shape)\n",
    "    print('Masks shape: ', masks.shape)\n",
    "    # print('Image paths: ', img_paths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.mps.is_available():  # Apple M-series of chips\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epoch_to_load = 93\n",
    "model = UNet(4).to(device=device)\n",
    "modelName = 'Test_Model'\n",
    "# model.load_state_dict(torch.load(f\"./models/{modelName}/{epoch_to_load}_Epoch\"))\n",
    "\n",
    "model.load_state_dict(torch.load(\"save/DSC-CE/61_Epoch-0.3DSC+0.7CE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dice_score_array = metrics.Dice_score_class(model, test_loader, device)\n",
    "iou_score_array = metrics.Jaccard_score_class(model, test_loader, device)\n",
    "hsd_score_array = metrics.hsd_score_class(model, test_loader, device)\n",
    "precision_score_array = metrics.precision_class(model, test_loader, device)\n",
    "recall_score_array = metrics.recall_class(model, test_loader, device)\n",
    "f1_score_array = metrics.f1_score_class(model, test_loader, device)\n",
    "auc_score_array = metrics.auc_coeff_class(model, test_loader, device)\n",
    "accuracy_score_array = metrics.accuracy_class(model, test_loader, device)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice score:  [0.9937838100803682, 0.42750724548692304, 0.4680207024956805, 0.6301194251136166]\n",
      "[0.42750724548692304, 0.4680207024956805, 0.6301194251136166]\n"
     ]
    }
   ],
   "source": [
    "print('Dice score: ', dice_score_array)\n",
    "print(dice_score_array[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_mean = np.mean(dice_score_array)\n",
    "iou_mean = np.mean(iou_score_array)\n",
    "hsd_mean = np.mean(hsd_score_array)\n",
    "precision_mean = np.mean(precision_score_array)\n",
    "recall_mean = np.mean(recall_score_array)\n",
    "f1_mean = np.mean(f1_score_array)\n",
    "auc_mean = np.mean(auc_score_array)\n",
    "accuracy_mean = np.mean(accuracy_score_array)\n",
    "\n",
    "dice_mean_without_bg = np.mean(dice_score_array[1:])\n",
    "iou_mean_without_bg = np.mean(iou_score_array[1:])\n",
    "hsd_mean_without_bg = np.mean(hsd_score_array[1:])\n",
    "precision_mean_without_bg = np.mean(precision_score_array[1:])\n",
    "recall_mean_without_bg = np.mean(recall_score_array[1:])\n",
    "f1_mean_without_bg = np.mean(f1_score_array[1:])\n",
    "auc_mean_without_bg = np.mean(auc_score_array[1:])\n",
    "accuracy_mean_without_bg = np.mean(accuracy_score_array[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# building the data frame \n",
    "data = {'Dice': dice_mean, 'IoU': iou_mean, 'HSD': hsd_mean, 'Precision': precision_mean, 'Recall': recall_mean, 'F1': f1_mean, 'AUC': auc_mean, 'Accuracy': accuracy_mean}\n",
    "data_without_bg = {'Dice': dice_mean_without_bg, 'IoU': iou_mean_without_bg, 'HSD': hsd_mean_without_bg, 'Precision': precision_mean_without_bg, 'Recall': recall_mean_without_bg, 'F1': f1_mean_without_bg, 'AUC': auc_mean_without_bg, 'Accuracy': accuracy_mean_without_bg}\n",
    "\n",
    "# Create DataFrame\n",
    "df_mean = pd.DataFrame(data, index =['Mean'])\n",
    "df_without_bg = pd.DataFrame(data_without_bg, index =['Mean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dice     IoU       HSD  Precision    Recall        F1       AUC  \\\n",
      "Mean  0.629858  0.5422  18.08874   0.652928  0.645326  0.620209  0.744082   \n",
      "\n",
      "      Accuracy  \n",
      "Mean  0.991937  \n"
     ]
    }
   ],
   "source": [
    "print(df_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dice       IoU        HSD  Precision    Recall        F1       AUC  \\\n",
      "Mean  0.508549  0.393751  13.503145   0.539033  0.529358  0.495671  0.714126   \n",
      "\n",
      "      Accuracy  \n",
      "Mean    0.9933  \n"
     ]
    }
   ],
   "source": [
    "print(df_without_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Dice       IoU        HSD  Precision    Recall        F1  \\\n",
      "Background  0.993784  0.987547  31.845524   0.994614  0.993230  0.993820   \n",
      "RV          0.427507  0.335838  40.509435   0.455080  0.463785  0.413471   \n",
      "Myo         0.468021  0.329313   0.000000   0.508702  0.473089  0.459968   \n",
      "LV          0.630119  0.516104   0.000000   0.653316  0.651199  0.613575   \n",
      "\n",
      "                 AUC  Accuracy  \n",
      "Background  0.833950  0.987846  \n",
      "RV          0.636150  0.991724  \n",
      "Myo         0.705835  0.992193  \n",
      "LV          0.800395  0.995984  \n"
     ]
    }
   ],
   "source": [
    "data_by_class = {'Dice': dice_score_array, 'IoU': iou_score_array, 'HSD': hsd_score_array, 'Precision': precision_score_array, 'Recall': recall_score_array, 'F1': f1_score_array, 'AUC': auc_score_array, 'Accuracy': accuracy_score_array}\n",
    "index = ['Background', 'RV', 'Myo', 'LV']\n",
    "df_by_class = pd.DataFrame(data_by_class, index=index)\n",
    "print(df_by_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv \n",
    "df_mean.to_csv('mean_metrics.csv')\n",
    "df_without_bg.to_csv('mean_metrics_without_bg.csv')\n",
    "df_by_class.to_csv('metrics_by_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
